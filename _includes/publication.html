<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 800px;
			height: 800px;
			margin-left: 0px;
		}
		
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 260px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 900px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 950px;

		}
		.short div.sub-left, .short div.sub-right{
			height:160px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>

<h3><a name='publications'></a> Selected Publications <font size="3"></a></font>
	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/EvEnhancerV1.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media-body">
	 <p class="media-heading">
	  <strong><font size="2.5">EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events <br /></font></strong>
	   <font size="2.5">Shuoyan Wei, <strong>Feng Li</strong>*, Shengen Tang, Yao Zhao, Huihui Bai*<br /> </font>
	   <font size="2.5">IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025<br /></font>
	</p>
	</div>
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/ControlGIC.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	 <strong><font size="2.5">Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaption<br /></font></strong>
	  <font size="2.5">Anqi Li, <strong>Feng Li*</strong>, Yuxi Liu, Runmin Cong, Yao Zhao, Huihui Bai*<br /> </font>
	  <font size="2.5">International Conference on Learning Representations (<strong>ICLR</strong>), 2025<br /> </font> 
	   <font size="2.5"><a href=https://arxiv.org/pdf/2406.00758">[Arxiv]</a> </font>
	   <font size="2.5"><a href="https://github.com/lianqi1008/Control-GIC">[Code]</a> </font>
	</p>
	</div>
	</div>	
	</div>
	</div>
	
	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/SRConvNet.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	 <strong><font size="2.5">SRConvNet: A Transformer-Style ConvNet for Lightweight Image Super-Resolution<br /></font></strong>
	  <font size="2.5"><strong>Feng Li</strong>, Runmin Cong*, Jingjing Wu, Huihui Bai, Meng Wang, Yao Zhao<br /> </font>
	   <font size="2.5">International Journal of Computer Vision (<strong>IJCV</strong>), 2025<br /></font>  
	  <font size="2.5"><a href="https://link.springer.com/content/pdf/10.1007/s11263-024-02147-y.pdf">[PDF]</a> </font>
	  <font size="2.5"><a href="https://github.com/lifengcs/SRConvNet">[Code]</a> </font>	
	</p>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/GranularDQ.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	 <strong><font size="2.5">Thinking in Granularity: Dynamic Quantization for Image Super-Resolution by Intriguing Multi-Granularity Clues<br /></font></strong>
	  <font size="2.5">Mingshen Wang, Zhao Zhang*, <strong>Feng Li*</strong>, Ke Xu, Kang Miao, Meng Wang<br /> </font>
	  <font size="2.5">AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025<br /> </font> 
	   <font size="2.5"><a href="https://arxiv.org/pdf/2409.14330">[Arxiv]</a> </font>
	   <font size="2.5"><a href="https://github.com/MmmingS/Granular-DQ.git">[Code]</a> </font>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/AENet.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	<strong><font size="2.5">Attend and Enrich: Enhanced Visual Prompt for Zero-Shot Learning <br /></font></strong>
	<font size="2.5">Man Liu, Huihui Bai*, <strong>Feng Li*</strong>, Chunjie Zhang, Yunchao Wei, Tat-Seng Chua, and Yao Zhao<br /> </font>
	<font size="2.5">AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025<br /></font> 
	<font size="2.5"><a href="https://arxiv.org/abs/2406.03032">[Arxiv]</a> </font>
	</p>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/sign-idd.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	<strong><font size="2.5">Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production <br /></font></strong>
	<font size="2.5">Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei*, <strong>Feng Li</strong>, Richang Hong*<br /> </font>
	<font size="2.5">AAAI Conference on Artificial Intelligence (<strong>AAAI, Oral</strong>), 2025<br /></font> 
	<font size="2.5"><a href="https://arxiv.org/pdf/2412.13609">[Arxiv]</a> </font>
	<font size="2.5"><a href="https://github.com/NaVi-start/Sign-IDD">[Code]</a> </font>
	</p>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/tpami.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	 <strong><font size="2.5">PSVMA+: Exploring Multi-granularity Semantic-visual Adaption for Generalized Zero-shot Learning<br /></font></strong>
	  <font size="2.5">Man Liu, Huihui Bai*, <strong>Feng Li</strong>*, Chunjie Zhang, Yunchao Wei, Meng Wang, Tat-Seng Chua, Yao Zhao<br /> </font>
	   <font size="2.5">IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024<br /></font>  
	</p>
	</div>
	</div>	
	</div>
	</div>
	
	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/blinddiff.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	<strong><font size="2.5">BlindDiff: Empowering Degradation Modelling in Diffusion Models for Blind Image Super-Resolution <br /></font></strong>
	<font size="2.5"><strong>Feng Li</strong>, Yixuan Wu, Zichao Liang, Runmin Cong, Huihui Bai*, Yao Zhao, and Meng Wang. <br /> </font>
	<font size="2.5">Science China-Information Science (<strong>SCIS</strong>), 2024<br /></font>  
	<font size="2.5"><a href="https://arxiv.org/abs/2403.10211">[Arxiv]</a> </font>
	<font size="2.5"><a href="https://github.com/lifengcs/BlindDiff">[Code]</a> </font>
	</p>
 
	</p>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/psvma.png" width="240" height="120">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	  <font size="2.5"><strong>Progressive Semantic-Visual Mutual adaption for Generalized Zero-Shot Learning</strong><br /></font>
	   <font size="2.5">Man Liu, <strong>Feng Li</strong>, Chunjie Zhang, Yunchao Wei, Huihui Bai*, and Yao Zhao<br /></font>
	   <font size="2.5">IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR, Top 10% highlight</strong>), 2023<br /></font>
	   <font size="2.5"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.pdf">[PDF]</a></font>
	  <font size="2.5"><a href="https://github.com/ManLiuCoder/PSVMA">[Code]</a></font>
	</p>
	</div>
	</div>	
	</div>
	</div>

	<div class="paper short">
	<div class="sub-left">
	<span></span>
	<img src="assets/images/fdsr.png" width="240" height="110">
	</div>
	<div class="sub-right">
	<div class="media">
	 <div class="media-body">
	<p class="media-heading">
	  <font size="2.5"><strong>Towards Complete Scene and Regular Shape for Distortion Rectification by Curve-aware Extrapolation</strong><br /></font>
	  <font size="2.5">Kang Liao, Chunyu Lin, Yunchao Wei, <strong>Feng Li</strong>, Shangrong Yang, Yao Zhao<br /></font>
	   <font size="2.5">IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021<br /></font>
	  <font size="2.5"> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Towards_Complete_Scene_and_Regular_Shape_for_Distortion_Rectification_by_ICCV_2021_paper.pdf">[PDF]</a></font>
	</p>

	</div>
	</div>	
	</div>
	</div>

	
	
	
<h4>


	
